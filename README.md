# Spotless Recommender
> ***Real-Time Nearby Food Recommendation***

## Introduction

This is a project I completed during the Insight Data Engineering program (New York, Fall 2018). This project aims building data pipeline for integrating data from Yelp open review dataset and sanitary inspections dataset from government. This project consists of four main components: batch processing part for
integrating and transforming data from different sources, machine learning part for collaborative filtering with user's rating and dining history data, real-time streaming part for handling requests from simulated users with locations, and finally visualization part of website. Raw data would be processed and
saved into database, then loaded into memory for real-time recommendation. Visit [spotlessrecommender](http://spotlessrecommender.site) to see it in action (or watch it [here](https://www.youtube.com/watch?v=pI_qaubQ47Q)).

## Problem:

Getting nearby food with app is quite popular nowadays and how to make recommendation has become a practical problem for restaurant rating app like Yelp. On the other side, people cares more about food sanitary and cleanliness, however ratings and reviews are based more on taste. As a customer,
you can never image what happened in kitchen. The situation is even worse in NYC, as many kitchen locates on basement and utilities are really old. In this scenario, how to make cleaner and better recommendation for customer would be essential but also challenging. How to integrate data from distinct
sources, how to utilize machine learning to improve quality of recommendation and how to send back results within seconds as no one want to wait too long for recommendation, this project is working on solve these issues.

## Tools and Technologies:

  1. SparkMLlib, SparkSQL, Streaming

  2. AWS S3, AWS RDS

  3. Apache Kafka

  4. Flask

  5. Airflow

-----------------

![alt text](https://github.com/waveambi/Insight_Recommendation/raw/master/docs/map.png "Food Map")

> * Restaurant scores are calculated with Yelp ratings, reviews sentiments and sanitary inspection results, simulated user requests are generated with real-time location to **Recommender**, and they would receive top 5 nearby restaurant recommendations. Nearby restaurant are defined as walking distance
 within 2 minutes and map are segmentated into 200m x 200m chunks of city street.*

## Pipeline:

-----------------

![alt text](https://github.com/waveambi/Insight_Recommendation/raw/master/docs/Pipeline.png "Recommender Pipeline")

***Batch Job***: download yelp ratings and reviews from Kaggle and sanitary inspections data from OpenData. Those are ingested from S3 bucket into Spark, which computes top-n nearby recommendation within every district of the grid, and results are writed into the AWS RDS Postgresql database. This process is scheduled and automated with Airflow.

***Streaming Job***: real-time simulated user requests and locations are ingested by Kafka and then consumed by Spark Streaming, where the data stream is joined with pre-calculated results from batch job to produce nearby recommendation for users.

## Data Sources

  1. Batch: [Yelp dataset by Kaggle](https://www.kaggle.com/yelp-dataset/yelp-dataset) contains restaurant data, user ratings and reviews data.

  2. Batch: [Sanitary Inspections Dataset](https://opendata.lasvegasnevada.gov/Public-Safety/Restaurant-Inspections/q8ye-5kwk) contains restaurants sanitary inspections data from 2010 to 2018

  2. Streaming: generated by simulator within this respiratory (streamed at 500~1,000 requests/s).

## Environment Setup

Install and configure [AWS CLI](https://aws.amazon.com/cli/) and [Pegasus](https://github.com/InsightDataScience/pegasus) on your local machine, and clone this repository using
`https://github.com/waveambi/Insight_Recommendation`.

AWS Tip: Add your local IP to your AWS VPC inbound rules

Pegasus Tip: follow the notes in docs/pegasus_setup.odt to configure Pegasus

#### CLUSTER STRUCTURE:

To reproduce my environment, 11 m4.large AWS EC2 instances are needed:

- (6 nodes) Spark Cluster - Batch and Streaming
- (4 nodes) Kafka Cluster
- Flask Node
- AWS RDS Postgresql node

To create the clusters, put the appropriate `master.yml` and `workers.yml` files in each `cluster_setup/<clustername>` folder (following the template in `cluster_setup/dummy.yml.template`), list all the necesary software in `cluster_setup/<clustername>/install.sh`, and run the `cluster_setup/create-clusters.sh` script.

After that, **Recommender** need to be cloned to the clusters (with necessary .jar files downloaded and required Python packages installed), and any node's address from cluster *some-cluster-name* will be saved as the environment variable SOME_CLUSTER_NAME_$i on every master, where $i = 0, 1, 2, ...

#### Airflow setup

The Apache Airflow scheduler can be installed on the master node of *spark-cluster*. Follow the instructions to launch the Airflow server.


#### PostgreSQL setup
The PostgreSQL database sits on AWS RDS *Postgresql* instance.

#### Configurations
Configuration settings for Kafka, Streaming, PostgreSQL, AWS S3 bucket for the data are stored in the respective files in `config/` folder.

Replace the settings in `config/s3bucket.config.` with the names and paths for your S3 bucket.

## Running Recommender

#### Schedule the Batch Job
Running `airflow/schedule.sh` on the master of *spark-cluster* will add the batch job to the scheduler. The batch job is set to execute every 24 hours, and it can be started and monitored from the Airflow GUI at `http://$SPARK_CLUSTER_0:8081`.
Also batch job could be running manualy with `spark-batch-run.sh` and `spark-batch-machine-learning.sh` with this respirotary.

#### Start the Streaming Job
Execute `./spark-streaming-run.sh` on the master of *spark-cluster* (preferably after batch job).

#### Start streaming messages with Kafka
Execute `./kafka-run --produce` on the master of *kafka-cluster* to start simulating real-time user requests.
If the topic does not exist, run `./kafka-run --create`. To describe existing topic, run `./kafka-run --describe`.
It is also possible to delete inactive topic using the option `--delete`, or view the messages in the topic with the option `--console-consume`.

#### Flask
Execute `./run.py` within flask folder on the master of *flask-node*, use http://spotlessrecommender.site to redirect to flask EC2 instance.
The default port number is 5000.
